{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from _base_network import _baseNetwork\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,1,0,1,1])\n",
    "b = np.array([1,1,0,1,0])\n",
    "\n",
    "np.sum((a == b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST_CSV/mnist_test.csv',header=None,delimiter=',')\n",
    "X = data.loc[:,1:].to_numpy()\n",
    "y = data[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e0a9e1150>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM20lEQVR4nO3dXahc9bnH8d/vpCmI6UXiS9ik0bTBC8tBEo1BSCxbQktOvIjFIM1FyYHi7kWUFkuo2It4WaQv1JvALkrTkmMJpGoQscmJxVDU4o5Es2NIjCGaxLxYIjQRJMY+vdjLso0za8ZZa2ZN8nw/sJmZ9cya9bDMz7VmvczfESEAV77/aroBAINB2IEkCDuQBGEHkiDsQBJfGeTCbHPoH+iziHCr6ZW27LZX2j5o+7Dth6t8FoD+cq/n2W3PkHRI0nckHZf0mqS1EfFWyTxs2YE+68eWfamkwxFxJCIuSPqTpNUVPg9AH1UJ+zxJx6a9Pl5M+xzbY7YnbE9UWBaAivp+gC4ixiWNS+zGA02qsmU/IWn+tNdfL6YBGEJVwv6apJtsf8P2VyV9X9L2etoCULeed+Mj4qLtByT9RdIMSU9GxP7aOgNQq55PvfW0ML6zA33Xl4tqAFw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9Dw+uyTZPirpnKRPJV2MiCV1NAWgfpXCXrgrIv5Rw+cA6CN244EkqoY9JO2wvcf2WKs32B6zPWF7ouKyAFTgiOh9ZnteRJywfb2knZIejIjdJe/vfWEAuhIRbjW90pY9Ik4Uj2ckPS1paZXPA9A/PYfd9tW2v/bZc0nflTRZV2MA6lXlaPxcSU/b/uxz/i8iXqilKwC1q/Sd/UsvjO/sQN/15Ts7gMsHYQeSIOxAEoQdSIKwA0nUcSNMCmvWrGlbu//++0vnff/990vrH3/8cWl9y5YtpfVTp061rR0+fLh0XuTBlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuCuty4dOXKkbW3BggWDa6SFc+fOta3t379/gJ0Ml+PHj7etPfbYY6XzTkxcvr+ixl1vQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97N3qeye9VtuuaV03gMHDpTWb7755tL6rbfeWlofHR1tW7vjjjtK5z127Fhpff78+aX1Ki5evFha/+CDD0rrIyMjPS/7vffeK61fzufZ22HLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD/7FWD27Nlta4sWLSqdd8+ePaX122+/vZeWutLp9/IPHTpUWu90/cKcOXPa1tavX18676ZNm0rrw6zn+9ltP2n7jO3JadPm2N5p++3isf2/NgBDoZvd+N9LWnnJtIcl7YqImyTtKl4DGGIdwx4RuyWdvWTyakmbi+ebJd1Tb1sA6tbrtfFzI+Jk8fyUpLnt3mh7TNJYj8sBUJPKN8JERJQdeIuIcUnjEgfogCb1eurttO0RSSoez9TXEoB+6DXs2yWtK56vk/RsPe0A6JeO59ltPyVpVNK1kk5L2ijpGUlbJd0g6V1J90XEpQfxWn0Wu/Ho2r333lta37p1a2l9cnKybe2uu+4qnffs2Y7/nIdWu/PsHb+zR8TaNqUVlToCMFBcLgskQdiBJAg7kARhB5Ig7EAS3OKKxlx//fWl9X379lWaf82aNW1r27ZtK533csaQzUByhB1IgrADSRB2IAnCDiRB2IEkCDuQBEM2ozGdfs75uuuuK61/+OGHpfWDBw9+6Z6uZGzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ7mdHXy1btqxt7cUXXyydd+bMmaX10dHR0vru3btL61cq7mcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4nx19tWrVqra1TufRd+3aVVp/5ZVXeuopq45bdttP2j5je3LatEdtn7C9t/hr/18UwFDoZjf+95JWtpj+m4hYVPw9X29bAOrWMewRsVvS2QH0AqCPqhyge8D2m8Vu/ux2b7I9ZnvC9kSFZQGoqNewb5K0UNIiSScl/ardGyNiPCKWRMSSHpcFoAY9hT0iTkfEpxHxL0m/k7S03rYA1K2nsNsemfbye5Im270XwHDoeJ7d9lOSRiVda/u4pI2SRm0vkhSSjkr6Uf9axDC76qqrSusrV7Y6kTPlwoULpfNu3LixtP7JJ5+U1vF5HcMeEWtbTH6iD70A6CMulwWSIOxAEoQdSIKwA0kQdiAJbnFFJRs2bCitL168uG3thRdeKJ335Zdf7qkntMaWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYMhmlLr77rtL688880xp/aOPPmpbK7v9VZJeffXV0jpaY8hmIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC+9mTu+aaa0rrjz/+eGl9xowZpfXnn28/5ifn0QeLLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH97Fe4TufBO53rvu2220rr77zzTmm97J71TvOiNz3fz257vu2/2n7L9n7bPy6mz7G90/bbxePsupsGUJ9uduMvSvppRHxL0h2S1tv+lqSHJe2KiJsk7SpeAxhSHcMeEScj4vXi+TlJByTNk7Ra0ubibZsl3dOnHgHU4EtdG297gaTFkv4uaW5EnCxKpyTNbTPPmKSxCj0CqEHXR+Ntz5K0TdJPIuKf02sxdZSv5cG3iBiPiCURsaRSpwAq6SrstmdqKuhbIuLPxeTTtkeK+oikM/1pEUAdOu7G27akJyQdiIhfTyttl7RO0i+Kx2f70iEqWbhwYWm906m1Th566KHSOqfXhkc339mXSfqBpH229xbTHtFUyLfa/qGkdyXd15cOAdSiY9gj4m+SWp6kl7Si3nYA9AuXywJJEHYgCcIOJEHYgSQIO5AEPyV9Bbjxxhvb1nbs2FHpszds2FBaf+655yp9PgaHLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59ivA2Fj7X/264YYbKn32Sy+9VFof5E+Roxq27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZLwPLly8vrT/44IMD6gSXM7bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEN+Ozz5f0B0lzJYWk8Yj4re1HJd0v6YPirY9ExPP9ajSzO++8s7Q+a9asnj+70/jp58+f7/mzMVy6uajmoqSfRsTrtr8maY/tnUXtNxHxy/61B6Au3YzPflLSyeL5OdsHJM3rd2MA6vWlvrPbXiBpsaS/F5MesP2m7Sdtz24zz5jtCdsT1VoFUEXXYbc9S9I2ST+JiH9K2iRpoaRFmtry/6rVfBExHhFLImJJ9XYB9KqrsNueqamgb4mIP0tSRJyOiE8j4l+Sfidpaf/aBFBVx7DbtqQnJB2IiF9Pmz4y7W3fkzRZf3sA6tLN0fhlkn4gaZ/tvcW0RySttb1IU6fjjkr6UR/6Q0VvvPFGaX3FihWl9bNnz9bZDhrUzdH4v0lyixLn1IHLCFfQAUkQdiAJwg4kQdiBJAg7kARhB5LwIIfctc34vkCfRUSrU+Vs2YEsCDuQBGEHkiDsQBKEHUiCsANJEHYgiUEP2fwPSe9Oe31tMW0YDWtvw9qXRG+9qrO3G9sVBnpRzRcWbk8M62/TDWtvw9qXRG+9GlRv7MYDSRB2IImmwz7e8PLLDGtvw9qXRG+9GkhvjX5nBzA4TW/ZAQwIYQeSaCTstlfaPmj7sO2Hm+ihHdtHbe+zvbfp8emKMfTO2J6cNm2O7Z223y4eW46x11Bvj9o+Uay7vbZXNdTbfNt/tf2W7f22f1xMb3TdlfQ1kPU28O/stmdIOiTpO5KOS3pN0tqIeGugjbRh+6ikJRHR+AUYtr8t6bykP0TEfxfTHpN0NiJ+UfyPcnZE/GxIentU0vmmh/EuRisamT7MuKR7JP2vGlx3JX3dpwGstya27EslHY6IIxFxQdKfJK1uoI+hFxG7JV06JMtqSZuL55s19Y9l4Nr0NhQi4mREvF48Pyfps2HGG113JX0NRBNhnyfp2LTXxzVc472HpB2299gea7qZFuZGxMni+SlJc5tspoWOw3gP0iXDjA/Nuutl+POqOED3Rcsj4lZJ/yNpfbG7OpRi6jvYMJ077WoY70FpMcz4fzS57nod/ryqJsJ+QtL8aa+/XkwbChFxong8I+lpDd9Q1Kc/G0G3eDzTcD//MUzDeLcaZlxDsO6aHP68ibC/Jukm29+w/VVJ35e0vYE+vsD21cWBE9m+WtJ3NXxDUW+XtK54vk7Ssw328jnDMox3u2HG1fC6a3z484gY+J+kVZo6Iv+OpJ830UObvr4p6Y3ib3/TvUl6SlO7dZ9o6tjGDyVdI2mXpLcl/b+kOUPU2x8l7ZP0pqaCNdJQb8s1tYv+pqS9xd+qptddSV8DWW9cLgskwQE6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji3y9hG/l2EQpSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y[0])\n",
    "plt.imshow(np.reshape(X[0],(28,28)),'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]\n",
      " [31 32 33 34 35 36 37 38 39 40]\n",
      " [41 42 43 44 45 46 47 48 49 50]]\n"
     ]
    }
   ],
   "source": [
    "yt = [0,2,3,4,5]\n",
    "pred = np.arange(1,51)\n",
    "pred = np.reshape(pred,(5,10))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13, 24, 35, 46])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[range(len(yt)),yt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9, -8, -7, -6, -5, -4, -3, -2, -1,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "a-np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 4. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 5. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# np.eye(10)\n",
    "class_labels = [i for i in range(10)]\n",
    "\n",
    "one_hot = np.eye(10)[np.vectorize(lambda c: class_labels[c])(yt).reshape(-1)]\n",
    "for ty in range(len(yt)):\n",
    "    one_hot[ty] = yt[ty] * one_hot[ty]\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseOptimizer:\n",
    "    def __init__(self, learning_rate=1e-4, reg=1e-3):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "\n",
    "    def update(self, model):\n",
    "        pass\n",
    "\n",
    "    def apply_regularization(self, model):\n",
    "        \"\"\"\n",
    "        Apply L2 penalty to the model. Update the gradient dictionary in the model\n",
    "        :param model: The model with gradients\n",
    "        :return: None, but the gradient dictionary of the model should be updated\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Apply L2 penalty to model weights based on the regularization       #\n",
    "        #       coefficient                                                         #\n",
    "        #############################################################################\n",
    "        \n",
    "        \n",
    "        self.penalty = 1/2*self.reg*np.sum(np.square(model.weights['W1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(_BaseOptimizer):\n",
    "    def __init__(self, learning_rate=1e-4, reg=1e-3):\n",
    "        super().__init__(learning_rate, reg)\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"\n",
    "        Update model weights based on gradients\n",
    "        :param model: The model to be updated\n",
    "        :return: None, but the model weights should be updated\n",
    "        \"\"\"\n",
    "        self.apply_regularization(model)\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Update model weights based on the learning rate and gradients       #\n",
    "        #############################################################################\n",
    "        model.weights['W1'] = model.weights['W1'] - self.learning_rate * (model.gradients + self.penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1/5 === Accuracy: 0.0, Loss: 6.0980030818603455\n",
      "\n",
      "Epoch # 2/5 === Accuracy: 0.0, Loss: 6.082539415919048\n",
      "\n",
      "Epoch # 3/5 === Accuracy: 0.4, Loss: 5.173443585944618\n",
      "\n",
      "Epoch # 4/5 === Accuracy: 0.0, Loss: 8.744642014741991\n",
      "\n",
      "Epoch # 5/5 === Accuracy: 0.0, Loss: 8.404311710212468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxRegression(_baseNetwork):\n",
    "    def __init__(self, input_size=28 * 28, num_classes=10):\n",
    "        \"\"\"\n",
    "        A single layer softmax regression. The network is composed by:\n",
    "        a linear layer without bias => (activation) => Softmax\n",
    "        :param input_size: the input dimension\n",
    "        :param num_classes: the number of classes in total\n",
    "        \"\"\"\n",
    "        super().__init__(input_size, num_classes)\n",
    "        self._weight_init()\n",
    "\n",
    "    def _weight_init(self):\n",
    "        '''\n",
    "        initialize weights of the single layer regression network. No bias term included.\n",
    "        :return: None; self.weights is filled based on method\n",
    "        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n",
    "        '''\n",
    "        np.random.seed(1024)\n",
    "        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n",
    "        self.gradients = np.zeros((self.input_size, self.num_classes))\n",
    "                \n",
    "    def one_hot(self,y):\n",
    "        class_labels = [i for i in range(10)]\n",
    "        one_hot = np.eye(self.num_classes)[np.vectorize(lambda c: class_labels[c])(y).reshape(-1)]\n",
    "        for i in range(len(y)):\n",
    "            one_hot[i] = one_hot[i] * y[i]\n",
    "        return one_hot\n",
    "    \n",
    "    def softmax(self, scores):\n",
    "        f = np.exp(scores - np.max(scores))  # shift values\n",
    "        return f / np.sum(f)\n",
    "\n",
    "    def forward(self, X, y, mode='train'):\n",
    "        \"\"\"\n",
    "        Compute loss and gradients using softmax with vectorization.\n",
    "\n",
    "        :param X: a batch of image (N, 28x28)\n",
    "        :param y: labels of images in the batch (N,)\n",
    "        :return:\n",
    "            loss: the loss associated with the batch\n",
    "            accuracy: the accuracy of the batch\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        gradient = None\n",
    "        accuracy = None\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n",
    "        #    2) Compute the gradient of the loss with respect to the weights        #\n",
    "        # Hint:                                                                     #\n",
    "        #   Store your intermediate outputs before ReLU for backwards               #\n",
    "        #############################################################################\n",
    "\n",
    "        # Z = X * W\n",
    "        Z = np.matmul(X,self.weights['W1'])\n",
    "        A = self.ReLU(Z)\n",
    "        p = self.softmax(A)\n",
    "        \n",
    "        accuracy = self.compute_accuracy(p,y)\n",
    "        loss = self.cross_entropy_loss(p,y)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        if mode != 'train':\n",
    "            return loss, accuracy\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the backward process:                                     #\n",
    "        #        1) Compute gradients of each weight by chain rule                  #\n",
    "        #        2) Store the gradients in self.gradients                           #\n",
    "        #############################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        error = (1/len(y))*(self.one_hot(y)-p)\n",
    "        self.gradients = np.dot(error.T,X).T\n",
    "        \n",
    "        # self.weights['W1'] = self.weights['W1'] - self.lr * self.gradients\n",
    "        \n",
    "        \n",
    "\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def train(self,X,y,epochs,batch_size):\n",
    "        loss_arr = []\n",
    "        accuracy_arr = []\n",
    "        sgd = SGD(learning_rate=1e-5)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            X_batch = X[epoch*batch_size:epoch*batch_size+batch_size]\n",
    "            y_batch = y[epoch*batch_size:epoch*batch_size+batch_size]\n",
    "            \n",
    "            loss,accuracy = self.forward(X_batch,y_batch,'train')\n",
    "            print(f\"Epoch # {epoch+1}/{epochs} === Accuracy: {accuracy}, Loss: {loss}\")\n",
    "            \n",
    "            sgd.update(self)\n",
    "            \n",
    "            loss_arr.append(loss)\n",
    "            accuracy_arr.append(accuracy)\n",
    "            print(\"\")\n",
    "            \n",
    "        return loss_arr, accuracy_arr\n",
    "    \n",
    "data_size = 50\n",
    "batch_size = 5\n",
    "epochs = 5\n",
    "\n",
    "classifier = SoftmaxRegression()\n",
    "loss, accuracy = classifier.train(X[:data_size],y[:data_size],epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:5\n",
      "5:10\n",
      "10:15\n",
      "15:20\n",
      "20:25\n",
      "25:30\n",
      "30:35\n",
      "35:40\n",
      "40:45\n",
      "45:50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "for epoch in range(10):\n",
    "    print(f\"{epoch*batch_size}:{epoch*batch_size+batch_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1e5415e4709ab72d4f13ab1c40055a30ddcaa9a7af539e0f44a9c51bba5cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
