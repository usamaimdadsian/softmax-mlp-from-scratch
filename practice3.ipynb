{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "np.random.seed(1024)\n",
    "from models._base_network import _baseNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoLayerNet(_baseNetwork):\n",
    "    def __init__(self, input_size=28 * 28, num_classes=10, hidden_size=128):\n",
    "        super().__init__(input_size, num_classes)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self._weight_init()\n",
    "\n",
    "    def _weight_init(self):\n",
    "        \"\"\"\n",
    "        initialize weights of the network\n",
    "        :return: None; self.weights is filled based on method\n",
    "        - W1: The weight matrix of the first layer of shape (num_features, hidden_size)\n",
    "        - b1: The bias term of the first layer of shape (hidden_size,)\n",
    "        - W2: The weight matrix of the second layer of shape (hidden_size, num_classes)\n",
    "        - b2: The bias term of the second layer of shape (num_classes,)\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights['b1'] = np.zeros(self.hidden_size)\n",
    "        self.weights['b2'] = np.zeros(self.num_classes)\n",
    "        np.random.seed(1024)\n",
    "        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.hidden_size)\n",
    "        np.random.seed(1024)\n",
    "        self.weights['W2'] = 0.001 * np.random.randn(self.hidden_size, self.num_classes)\n",
    "\n",
    "        # initialize gradients to zeros\n",
    "        self.gradients['W1'] = np.zeros((self.input_size, self.hidden_size))\n",
    "        self.gradients['b1'] = np.zeros(self.hidden_size)\n",
    "        self.gradients['W2'] = np.zeros((self.hidden_size, self.num_classes))\n",
    "        self.gradients['b2'] = np.zeros(self.num_classes)\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, X, y, mode='train'):\n",
    "        \"\"\"\n",
    "        The forward pass of the two-layer net. The activation function used in between the two layers is sigmoid, which\n",
    "        is to be implemented in self.,sigmoid.\n",
    "        The method forward should compute the loss of input batch X and gradients of each weights.\n",
    "        Further, it should also compute the accuracy of given batch. The loss and\n",
    "        accuracy are returned by the method and gradients are stored in self.gradients\n",
    "\n",
    "        :param X: a batch of images (N, input_size)\n",
    "        :param y: labels of images in the batch (N,)\n",
    "        :param mode: if mode is training, compute and update gradients;else, just return the loss and accuracy\n",
    "        :return:\n",
    "            loss: the loss associated with the batch\n",
    "            accuracy: the accuracy of the batch\n",
    "            self.gradients: gradients are not explicitly returned but rather updated in the class member self.gradients\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        accuracy = None\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the forward process:                                      #\n",
    "        #        1) Call sigmoid function between the two layers for non-linearity  #\n",
    "        #        2) The output of the second layer should be passed to softmax      #\n",
    "        #        function before computing the cross entropy loss                   #\n",
    "        #    2) Compute Cross-Entropy Loss and batch accuracy based on network      #\n",
    "        #       outputs                                                             #\n",
    "        #############################################################################\n",
    "        \n",
    "        Z1 = np.dot(X,self.weights['W1']) + self.weights['b1']\n",
    "        A1 = self.sigmoid(Z1)\n",
    "        \n",
    "    \n",
    "        \n",
    "        Z2 = np.dot(A1, self.weights['W2']) + self.weights['b2']\n",
    "        A2 = self.softmax(Z2)\n",
    "        \n",
    "        accuracy = self.compute_accuracy(A2,y)\n",
    "        loss = self.cross_entropy_loss(A2,y)\n",
    "\n",
    "\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the backward process:                                     #\n",
    "        #        1) Compute gradients of each weight and bias by chain rule         #\n",
    "        #        2) Store the gradients in self.gradients                           #\n",
    "        #    HINT: You will need to compute gradients backwards, i.e, compute       #\n",
    "        #          gradients of W2 and b2 first, then compute it for W1 and b1      #\n",
    "        #          You may also want to implement the analytical derivative of      #\n",
    "        #          the sigmoid function in self.sigmoid_dev first                   #\n",
    "        #############################################################################\n",
    "\n",
    "        error = A2\n",
    "        error[range(len(y)),y] -= 1\n",
    "        \n",
    "        self.gradients['W2'] = 1/len(y) * np.dot(A1.T,error)\n",
    "        self.gradients['b2'] = 1/len(y) * np.sum(error,axis=0)\n",
    "        \n",
    "        dhidden = np.dot(1/len(y) *error,self.weights['W2'].T)\n",
    "        diff_sigmoid = dhidden*self.sigmoid_dev(Z1)\n",
    "        \n",
    "        self.gradients['W1'] = np.dot(X.T,diff_sigmoid)\n",
    "        self.gradients['b1'] = np.sum(diff_sigmoid,axis=0)\n",
    "\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "test_batch = np.load('tests/softmax_grad_check/test_batch.npy')\n",
    "test_label = np.load('tests/softmax_grad_check/test_label.npy')\n",
    "\n",
    "model = TwoLayerNet(hidden_size=128)\n",
    "# expected_loss = 2.30285\n",
    "w1_grad_expected = np.load('tests/twolayer_grad_check/w1.npy')\n",
    "b1_grad_expected = np.load('tests/twolayer_grad_check/b1.npy')\n",
    "w2_grad_expected = np.load('tests/twolayer_grad_check/w2.npy')\n",
    "b2_grad_expected = np.load('tests/twolayer_grad_check/b2.npy')\n",
    "\n",
    "loss, _ = model.forward(test_batch, test_label, mode='train')\n",
    "\n",
    "\n",
    "tn = TestNetwork()\n",
    "tn.setUp()\n",
    "tn.test_two_layer_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNetwork(unittest.TestCase):\n",
    "    \"\"\" The class containing all test cases for this assignment\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Define the functions to be tested here.\"\"\"\n",
    "        self.test_batch = np.load('tests/softmax_grad_check/test_batch.npy')\n",
    "        self.test_label = np.load('tests/softmax_grad_check/test_label.npy')\n",
    "\n",
    "    def test_one_layer_softmax_relu(self):\n",
    "        model = SoftmaxRegression()\n",
    "        expected_loss = 2.3029\n",
    "        expected_grad = np.load('tests/softmax_grad_check/softmax_relu_grad.npy')\n",
    "        loss, _ = model.forward(self.test_batch, self.test_label, mode='train')\n",
    "        w_grad = model.gradients['W1']\n",
    "        self.assertAlmostEqual(expected_loss, loss, places=5)\n",
    "        diff = np.sum(np.abs(expected_grad - w_grad))\n",
    "        self.assertAlmostEqual(diff, 0)\n",
    "\n",
    "    def test_two_layer_net(self):\n",
    "        model = TwoLayerNet(hidden_size=128)\n",
    "        expected_loss = 2.30285\n",
    "        w1_grad_expected = np.load('tests/twolayer_grad_check/w1.npy')\n",
    "        b1_grad_expected = np.load('tests/twolayer_grad_check/b1.npy')\n",
    "        w2_grad_expected = np.load('tests/twolayer_grad_check/w2.npy')\n",
    "        b2_grad_expected = np.load('tests/twolayer_grad_check/b2.npy')\n",
    "\n",
    "        loss, _ = model.forward(self.test_batch, self.test_label, mode='train')\n",
    "\n",
    "        self.assertAlmostEqual(expected_loss, loss, places=5)\n",
    "\n",
    "        self.assertAlmostEqual(np.sum(np.abs(w1_grad_expected - model.gradients['W1'])), 0)\n",
    "        self.assertAlmostEqual(np.sum(np.abs(b1_grad_expected - model.gradients['b1'])), 0)\n",
    "        self.assertAlmostEqual(np.sum(np.abs(w2_grad_expected - model.gradients['W2'])), 0)\n",
    "        self.assertAlmostEqual(np.sum(np.abs(b2_grad_expected - model.gradients['b2'])), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(_baseNetwork):\n",
    "    def __init__(self, input_size=28 * 28, num_classes=10):\n",
    "        \"\"\"\n",
    "        A single layer softmax regression. The network is composed by:\n",
    "        a linear layer without bias => (activation) => Softmax\n",
    "        :param input_size: the input dimension\n",
    "        :param num_classes: the number of classes in total\n",
    "        \"\"\"\n",
    "        super().__init__(input_size, num_classes)\n",
    "        self._weight_init()\n",
    "\n",
    "    def _weight_init(self):\n",
    "        '''\n",
    "        initialize weights of the single layer regression network. No bias term included.\n",
    "        :return: None; self.weights is filled based on method\n",
    "        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n",
    "        '''\n",
    "        np.random.seed(1024)\n",
    "        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n",
    "        self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))\n",
    "                \n",
    "    def one_hot(self,y):\n",
    "        class_labels = [i for i in range(10)]\n",
    "        one_hot = np.eye(self.num_classes)[np.vectorize(lambda c: class_labels[c])(y).reshape(-1)]\n",
    "        for i in range(len(y)):\n",
    "            one_hot[i] = one_hot[i] * y[i]\n",
    "        return one_hot\n",
    "    \n",
    "    # def softmax(self, scores):\n",
    "    #     f = np.exp(scores - np.max(scores))  # shift values\n",
    "    #     return f / np.sum(f)\n",
    "\n",
    "    def forward(self, X, y, mode='train'):\n",
    "        \"\"\"\n",
    "        Compute loss and gradients using softmax with vectorization.\n",
    "\n",
    "        :param X: a batch of image (N, 28x28)\n",
    "        :param y: labels of images in the batch (N,)\n",
    "        :return:\n",
    "            loss: the loss associated with the batch\n",
    "            accuracy: the accuracy of the batch\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        gradient = None\n",
    "        accuracy = None\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n",
    "        #    2) Compute the gradient of the loss with respect to the weights        #\n",
    "        # Hint:                                                                     #\n",
    "        #   Store your intermediate outputs before ReLU for backwards               #\n",
    "        #############################################################################\n",
    "\n",
    "        # Z = X * W\n",
    "        Z = np.matmul(X,self.weights['W1'])\n",
    "        A = self.ReLU(Z)\n",
    "        p = self.softmax(A)\n",
    "        \n",
    "        accuracy = self.compute_accuracy(p,y)\n",
    "        loss = self.cross_entropy_loss(p,y)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        if mode != 'train':\n",
    "            return loss, accuracy\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the backward process:                                     #\n",
    "        #        1) Compute gradients of each weight by chain rule                  #\n",
    "        #        2) Store the gradients in self.gradients                           #\n",
    "        #############################################################################\n",
    "        \n",
    "        error = p\n",
    "        error[range(len(y)),y] -= 1\n",
    "        \n",
    "        \n",
    "        # dA = p - y # derivative of softmax cross-entropy loss with respect to A\n",
    "        dZ = 1/len(y)*np.multiply(error, np.int64(A > 0)) # derivative of ReLU activation with respect to Z\n",
    "        dW = np.dot(X.T, dZ)\n",
    "        \n",
    "        self.gradients['W1'] += dW # update gradients\n",
    "        \n",
    "        \n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "tn = TestNetwork()\n",
    "tn.setUp()\n",
    "tn.test_one_layer_softmax_relu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1e5415e4709ab72d4f13ab1c40055a30ddcaa9a7af539e0f44a9c51bba5cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
