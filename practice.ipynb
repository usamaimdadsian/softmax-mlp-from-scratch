{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from _base_network import _baseNetwork\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1,0,1,1])\n",
    "b = np.array([1,1,0,1,0])\n",
    "\n",
    "np.sum((a == b))\n",
    "GRADIENT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST_CSV/mnist_test.csv',header=None,delimiter=',')\n",
    "X = data.loc[:,1:].to_numpy()\n",
    "y = data[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa865ab0280>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y[0])\n",
    "plt.imshow(np.reshape(X[0],(28,28)),'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]\n",
      " [31 32 33 34 35 36 37 38 39 40]\n",
      " [41 42 43 44 45 46 47 48 49 50]]\n"
     ]
    }
   ],
   "source": [
    "yt = [0,2,3,4,5]\n",
    "pred = np.arange(1,51)\n",
    "pred = np.reshape(pred,(5,10))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13, 24, 35, 46])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[range(len(yt)),yt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 784) (784, 10)\n",
      "(5, 10) (5,)\n",
      "3.6959928636697286\n",
      "hello g (5,) (784,)\n",
      "Shape: (5,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 121>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m classifier \u001b[39m=\u001b[39m SoftmaxRegression()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mtrain(X[:data_size],y[:data_size],epochs,batch_size)\n",
      "\u001b[1;32m/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb Cell 7\u001b[0m in \u001b[0;36mSoftmaxRegression.train\u001b[0;34m(self, X, y, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m X_batch \u001b[39m=\u001b[39m X[epoch\u001b[39m*\u001b[39mbatch_size:epoch\u001b[39m*\u001b[39mbatch_size\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m y_batch \u001b[39m=\u001b[39m y[epoch\u001b[39m*\u001b[39mbatch_size:epoch\u001b[39m*\u001b[39mbatch_size\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m loss,accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(X_batch,y_batch,\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch # \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m === Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m loss_arr\u001b[39m.\u001b[39mappend(loss)\n",
      "\u001b[1;32m/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb Cell 7\u001b[0m in \u001b[0;36mSoftmaxRegression.forward\u001b[0;34m(self, X, y, mode)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhello g\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_inv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m][:,class_],X)\u001b[39m.\u001b[39mshape,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m][:,class_]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mShape:\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mderivative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_inv,(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m][:,class_],X))\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients[:,class_] \u001b[39m=\u001b[39m dw_Z[class_] \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mderivative(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_inv,(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[\u001b[39m'\u001b[39;49m\u001b[39mW1\u001b[39;49m\u001b[39m'\u001b[39;49m][:,class_],X))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# self.gradients = dw_W\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usamaimdad/zodiacSpace/Programs/upwork/dhivya/practice.ipynb#W1sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (5,) "
     ]
    }
   ],
   "source": [
    "class SoftmaxRegression(_baseNetwork):\n",
    "    def __init__(self, input_size=28 * 28, num_classes=10):\n",
    "        \"\"\"\n",
    "        A single layer softmax regression. The network is composed by:\n",
    "        a linear layer without bias => (activation) => Softmax\n",
    "        :param input_size: the input dimension\n",
    "        :param num_classes: the number of classes in total\n",
    "        \"\"\"\n",
    "        super().__init__(input_size, num_classes)\n",
    "        self._weight_init()\n",
    "\n",
    "    def _weight_init(self):\n",
    "        '''\n",
    "        initialize weights of the single layer regression network. No bias term included.\n",
    "        :return: None; self.weights is filled based on method\n",
    "        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n",
    "        '''\n",
    "        np.random.seed(1024)\n",
    "        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n",
    "        self.gradients = np.zeros((self.input_size, self.num_classes))\n",
    "      \n",
    "    def derivative(self,y,x):\n",
    "        h = 1e-5\n",
    "        if len(x) == 2 and isinstance(x,tuple):\n",
    "            pos_change_param = (x[0]+h,x[1])\n",
    "            neg_change_param = (x[0]-h,x[1])\n",
    "            \n",
    "            pos_change = y(*pos_change_param)        \n",
    "            neg_change = y(*neg_change_param)\n",
    "        \n",
    "            return (pos_change - neg_change) / 2*h\n",
    "        else:\n",
    "            return (y(x+h) - y(x-h))/2*h\n",
    "        \n",
    "    def predict_inv(self,weights, X):\n",
    "        return np.matmul(weights,X.T)\n",
    "\n",
    "    def forward(self, X, y, mode='train'):\n",
    "        \"\"\"\n",
    "        Compute loss and gradients using softmax with vectorization.\n",
    "\n",
    "        :param X: a batch of image (N, 28x28)\n",
    "        :param y: labels of images in the batch (N,)\n",
    "        :return:\n",
    "            loss: the loss associated with the batch\n",
    "            accuracy: the accuracy of the batch\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        gradient = None\n",
    "        accuracy = None\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n",
    "        #    2) Compute the gradient of the loss with respect to the weights        #\n",
    "        # Hint:                                                                     #\n",
    "        #   Store your intermediate outputs before ReLU for backwards               #\n",
    "        #############################################################################\n",
    "\n",
    "        # Z = X * W\n",
    "        print(X.shape,self.weights['W1'].shape)\n",
    "        Z = np.matmul(X,self.weights['W1'])\n",
    "        A = self.ReLU(Z)\n",
    "        p = self.softmax(A)\n",
    "        \n",
    "        print(p.shape,y.shape)\n",
    "        # TODO find accuracy\n",
    "        # accuracy = np.sum((p == y)) / len(y)\n",
    "        loss = self.cross_entropy_loss(p,y)\n",
    "        print(loss)\n",
    "        \n",
    "\n",
    "\n",
    "        if mode != 'train':\n",
    "            return loss, accuracy\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO:                                                                     #\n",
    "        #    1) Implement the backward process:                                     #\n",
    "        #        1) Compute gradients of each weight by chain rule                  #\n",
    "        #        2) Store the gradients in self.gradients                           #\n",
    "        #############################################################################\n",
    "        \n",
    "        dw_A = self.derivative(self.cross_entropy_loss,(p,y)) * self.derivative(self.softmax,A)\n",
    "        dw_Z = dw_A * self.derivative(self.ReLU,Z)\n",
    "        \n",
    "        for class_ in range(self.num_classes):\n",
    "            print('hello g',self.predict_inv(self.weights['W1'][:,class_],X).shape,self.weights['W1'][:,class_].shape)\n",
    "            print('Shape:',self.derivative(self.predict_inv,(self.weights['W1'][:,class_],X)).shape)\n",
    "            self.gradients[:,class_] = dw_Z[class_] * self.derivative(self.predict_inv,(self.weights['W1'][:,class_],X))\n",
    "            \n",
    "            \n",
    "        \n",
    "        # self.gradients = dw_W\n",
    "        self.weights['W1'] = self.weights['W1'] - self.lr * self.gradients\n",
    "\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def train(self,X,y,epochs,batch_size,learning_rate=0.001):\n",
    "        loss_arr = []\n",
    "        accuracy_arr = []\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            X_batch = X[epoch*batch_size:epoch*batch_size+batch_size]\n",
    "            y_batch = y[epoch*batch_size:epoch*batch_size+batch_size]\n",
    "            \n",
    "            loss,accuracy = self.forward(X_batch,y_batch,'train')\n",
    "            print(f\"Epoch # {epoch+1}/{epochs} === Accuracy: {accuracy}, Loss: {loss}\")\n",
    "            \n",
    "            loss_arr.append(loss)\n",
    "            accuracy_arr.append(accuracy)\n",
    "            \n",
    "        return loss_arr, accuracy_arr\n",
    "    \n",
    "data_size = 50\n",
    "batch_size = 5\n",
    "epochs = 5\n",
    "\n",
    "classifier = SoftmaxRegression()\n",
    "loss, accuracy = classifier.train(X[:data_size],y[:data_size],epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:5\n",
      "5:10\n",
      "10:15\n",
      "15:20\n",
      "20:25\n",
      "25:30\n",
      "30:35\n",
      "35:40\n",
      "40:45\n",
      "45:50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "for epoch in range(10):\n",
    "    print(f\"{epoch*batch_size}:{epoch*batch_size+batch_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1e5415e4709ab72d4f13ab1c40055a30ddcaa9a7af539e0f44a9c51bba5cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
